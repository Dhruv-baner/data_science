{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1899d76b",
   "metadata": {},
   "source": [
    "### **1. Text Preprocessing and Representation** \n",
    "\n",
    "**Problem**: Computers don't understand text. We need to convert words to numbers that capture meaning \n",
    "\n",
    "**Tokenization:** Breaking Text into units called tokens, which are words, characters or subwords\n",
    "\n",
    "**Why it matters:** \n",
    "- ML models need numbers, not raw text\n",
    "- Need to define what a \"unit\" of meaning is\n",
    "- Can't process entire sentences as single entities\n",
    "- First step to counting/vectorizing words\n",
    "- Determines granularity of your analysis\n",
    "- **First step in any NLP pipeline. Defines our vocabulary + Affects everything downstream.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10d1dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"I love NLP! It's amazing.\"\n",
    "\n",
    "# Word tokenization\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)\n",
    "# Output: ['I', 'love', 'NLP', '!', 'It', \"'s\", 'amazing', '.']\n",
    "\n",
    "# Simple split (naive approach)\n",
    "simple_tokens = text.split()\n",
    "print(simple_tokens)\n",
    "# Output: ['I', 'love', 'NLP!', \"It's\", 'amazing.']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b32df9",
   "metadata": {},
   "source": [
    "#### **Lowercasing**\n",
    "\n",
    "**Why we do it (intuition):**\n",
    "\n",
    "- \"Apple\", \"apple\", \"APPLE\" → should usually be treated as same word\n",
    "- Reduces vocabulary size (fewer unique tokens)\n",
    "- Prevents model from thinking \"The\" and \"the\" are different\n",
    "- Simple way to normalize text\n",
    "\n",
    "**Trade-off:** Loses information. \"Apple\" (company) vs \"apple\" (fruit). \"US\" (country) vs \"us\" (pronoun). For most tasks, the simplification is worth it. For NER or cases where capitalization carries meaning, don't do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b3920c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Apple makes great products. I love apples.\"\n",
    "\n",
    "lowercase_text = text.lower()\n",
    "print(lowercase_text)\n",
    "# Output: \"apple makes great products. i love apples.\"\n",
    "\n",
    "# In a pipeline\n",
    "tokens = word_tokenize(text.lower())\n",
    "print(tokens)\n",
    "# Output: ['apple', 'makes', 'great', 'products', '.', 'i', 'love', 'apples', '.']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1cc46f",
   "metadata": {},
   "source": [
    "#### **Punctuations**\n",
    "\n",
    "**Why we do it:**\n",
    "\n",
    "- Punctuation often doesn't carry semantic meaning for simple tasks\n",
    "- Reduces noise in vocabulary (\"word\" vs \"word,\" vs \"word.\" are same)\n",
    "- Makes feature vectors cleaner for BoW/TF-IDF\n",
    "- \"Hello!\" and \"Hello\" should probably mean the same thing\n",
    "\n",
    "Trade-off: Modern models (BERT, GPT) actually learn from punctuation. \"Let's eat grandma\" vs \"Let's eat, grandma\" - punctuation changes meaning. \n",
    "\n",
    "**Decision Framework**:\n",
    "- For classical methods (BoW, TF-IDF) → remove it. \n",
    "- For deep learning → keep it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54be84b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "text = \"Hello, world! How are you?\"\n",
    "\n",
    "# Remove all punctuation\n",
    "no_punct = text.translate(str.maketrans('', '', string.punctuation))\n",
    "print(no_punct)\n",
    "# Output: \"Hello world How are you\"\n",
    "\n",
    "# Or with regex\n",
    "import re\n",
    "no_punct = re.sub(r'[^\\w\\s]', '', text)\n",
    "print(no_punct)\n",
    "# Output: \"Hello world How are you\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0e7f6a",
   "metadata": {},
   "source": [
    "#### **Stopwords**\n",
    "\n",
    "**What it is:** Common words that appear frequently but carry little meaningful information (the, is, at, which, on, a, an, etc.)\n",
    "\n",
    "**Why we do it:**\n",
    "\n",
    "- \"the\", \"is\", \"at\" appear in almost every document → don't help distinguish documents\n",
    "- Reduces dimensionality (smaller vocabulary)\n",
    "- Focuses on content words that carry actual meaning\n",
    "- Speeds up computation\n",
    "\n",
    "**Trade-off:** Can lose context. \"not good\" → remove \"not\" → becomes \"good\" (opposite meaning!). \n",
    "\n",
    "- For BoW/TF-IDF with limited compute → remove them. \n",
    "- For models with enough capacity (embeddings, transformers) → keep them, context matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da4263a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"This is a simple example showing stop word removal\"\n",
    "\n",
    "# Get English stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "tokens = word_tokenize(text.lower())\n",
    "print(\"Original:\", tokens)\n",
    "# Output: ['this', 'is', 'a', 'simple', 'example', 'showing', 'stop', 'word', 'removal']\n",
    "\n",
    "# Remove stop words\n",
    "filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "print(\"Filtered:\", filtered_tokens)\n",
    "# Output: ['simple', 'example', 'showing', 'stop', 'word', 'removal']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33010d6",
   "metadata": {},
   "source": [
    "#### **Stemming vs Lemmatization**\n",
    "What it is: Both reduce words to their root/base form, but use different approaches.\n",
    "- Stemming - crude chopping using rules\n",
    "- Lemmatization - dictionary lookup to find proper root\n",
    "\n",
    "**Why we do it**:\n",
    "- Groups related words together (\"running\", \"runs\", \"ran\" → all mean \"run\")\n",
    "- Reduces vocabulary size\n",
    "- Helps model see that different forms are the same concept\n",
    "- Improves generalization\n",
    "\n",
    "Trade-off: Stemming is fast but crude (can create non-words like \"studi\"). Lemmatization is accurate but slower. For search/IR → stemming fine. For tasks needing precision → lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3282e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "words = [\"running\", \"runs\", \"ran\", \"better\", \"studies\", \"caring\"]\n",
    "\n",
    "# Stemming\n",
    "stemmed = [stemmer.stem(word) for word in words]\n",
    "print(\"Stemmed:\", stemmed)\n",
    "# Output: ['run', 'run', 'ran', 'better', 'studi', 'care']\n",
    "\n",
    "# Lemmatization\n",
    "lemmatized = [lemmatizer.lemmatize(word, pos='v') for word in words]\n",
    "print(\"Lemmatized:\", lemmatized)\n",
    "# Output: ['run', 'run', 'run', 'better', 'study', 'care']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93365f2d",
   "metadata": {},
   "source": [
    "#### **N-Grams**\n",
    "\n",
    "What it is: Sequences of N consecutive words/tokens. Captures phrases instead of just individual words.\n",
    "\n",
    "- Unigram (1-gram): single words\n",
    "- Bigram (2-gram): pairs of consecutive words\n",
    "- Trigram (3-gram): triplets of consecutive words\n",
    "\n",
    "**Why we do it:**\n",
    "\n",
    "- Captures phrases and context (\"New York\" is one entity, not \"New\" + \"York\")\n",
    "\"not good\" has different meaning than \"good\" alone\n",
    "- Word order matters for meaning\n",
    "- Better features for classification\n",
    "\n",
    "**Trade-off:** Higher N = more context but exponentially larger vocabulary (sparse features). Bigrams usually sweet spot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce7255a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"New York is a great city\"\n",
    "tokens = word_tokenize(text.lower())\n",
    "\n",
    "# Unigrams\n",
    "unigrams = list(ngrams(tokens, 1))\n",
    "print(\"Unigrams:\", unigrams)\n",
    "# Output: [('new',), ('york',), ('is',), ('a',), ('great',), ('city',)]\n",
    "\n",
    "# Bigrams\n",
    "bigrams = list(ngrams(tokens, 2))\n",
    "print(\"Bigrams:\", bigrams)\n",
    "# Output: [('new', 'york'), ('york', 'is'), ('is', 'a'), ('a', 'great'), ('great', 'city')]\n",
    "\n",
    "# Trigrams\n",
    "trigrams = list(ngrams(tokens, 3))\n",
    "print(\"Trigrams:\", trigrams)\n",
    "# Output: [('new', 'york', 'is'), ('york', 'is', 'a'), ('is', 'a', 'great'), ('a', 'great', 'city')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591fd6ca",
   "metadata": {},
   "source": [
    "#### **Bag of Words**\n",
    "\n",
    "**What it is:** Represent text as a vector of word counts. Each position in vector = one word in vocabulary. Ignores word order completely.\n",
    "\n",
    "**Why we do it:**\n",
    "\n",
    "- Converts text into numbers that ML models can use\n",
    "- Simple, fast, interpretable\n",
    "- Each document becomes a fixed-length vector\n",
    "- Good baseline for classification tasks\n",
    "\n",
    "**Trade-off:** \n",
    "- Loses all word order (\"dog bites man\" = \"man bites man\" = \"bites dog man\"). - High dimensionality (one dimension per unique word). \n",
    "- No semantic understanding (\"king\" and \"queen\" are completely unrelated)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e944c9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "documents = [\n",
    "    \"I love cats\",\n",
    "    \"I love dogs\",\n",
    "    \"cats and dogs\"\n",
    "]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "bow_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
    "# Output: ['and' 'cats' 'dogs' 'love']\n",
    "\n",
    "print(\"BoW Matrix:\\n\", bow_matrix.toarray())\n",
    "# Output:\n",
    "# [[0 1 0 1]  <- \"I love cats\"\n",
    "#  [0 0 1 1]  <- \"I love dogs\"\n",
    "#  [1 1 1 0]] <- \"cats and dogs\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745acbe4",
   "metadata": {},
   "source": [
    "#### **TF-IDF**\n",
    "\n",
    "**What it is:** Improved version of BoW. Weights words by how important they are. Common words get lower weight, rare distinctive words get higher weight.\n",
    "\n",
    "**Formula:**\n",
    "- TF (Term Frequency) = how often word appears in document\n",
    "- IDF (Inverse Document Frequency) = log(total documents / documents containing word)\n",
    "- TF-IDF = TF × IDF\n",
    "\n",
    "**Why we do it:**\n",
    "\n",
    "- Downweights common useless words (\"the\", \"is\", \"and\")\n",
    "- Highlights distinctive/informative words\n",
    "- Better features than raw counts for classification\n",
    "- Captures document uniqueness\n",
    "\n",
    "**Trade-off:** Still no word order. Still no semantics. But much better than BoW for most tasks. Standard baseline for text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1c66c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "documents = [\n",
    "    \"the cat sat on the mat\",\n",
    "    \"the dog sat on the log\",\n",
    "    \"cats and dogs are enemies\"\n",
    "]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
    "print(\"TF-IDF Matrix:\\n\", tfidf_matrix.toarray())\n",
    "\n",
    "# \"the\" appears in doc 1 & 2 → low IDF → low TF-IDF\n",
    "# \"enemies\" appears in only doc 3 → high IDF → high TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5549b982",
   "metadata": {},
   "source": [
    "### **2. Word Embeddings**\n",
    "\n",
    "The motivation for moving beyond BoW/TF-IDF to dense vector representations where similar words have similar vectors.\n",
    "\n",
    "**Why we do it:**\n",
    "- Capture semantic meaning (similar words → similar vectors)\n",
    "- Reduce dimensionality (300 dims vs 50,000+ vocab size)\n",
    "- Enable mathematical operations on meaning (king - man + woman ≈ queen)\n",
    "- Transfer knowledge (pre-trained embeddings from huge corpora)\n",
    "- Foundation for modern NLP (BERT, GPT, etc.)\n",
    "\n",
    "**The Problem with BoW/TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562313f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In BoW/TF-IDF, each word is independent\n",
    "Vocabulary: [king, queen, man, woman, cat, dog]\n",
    "\n",
    "\"king\" = [1, 0, 0, 0, 0, 0]\n",
    "\"queen\" = [0, 1, 0, 0, 0, 0]\n",
    "\"cat\" = [0, 0, 0, 0, 1, 0]\n",
    "\n",
    "# Distance between \"king\" and \"queen\" = same as distance between \"king\" and \"cat\"\n",
    "# Model has no idea that king/queen are related!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a174bb9",
   "metadata": {},
   "source": [
    "**The Embedding Solution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a7ed69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words become dense vectors in continuous space\n",
    "\"king\" = [0.5, 0.8, 0.1, ...]  (e.g., 300 dimensions)\n",
    "\"queen\" = [0.48, 0.79, 0.12, ...]\n",
    "\"cat\" = [-0.2, 0.1, 0.9, ...]\n",
    "\n",
    "# Now: king and queen are CLOSE in vector space\n",
    "# Semantic similarity captured by geometric proximity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9b25b5",
   "metadata": {},
   "source": [
    "#### **WORD2VEC**\n",
    "\n",
    "**What it is:** A method to learn word embeddings by predicting context. \n",
    "\n",
    "Two approaches: CBOW (predict word from context) and Skip-gram (predict context from word).\n",
    "\n",
    "**Two Architectures:**\n",
    "\n",
    "1. CBOW (Continuous Bag of Words):\n",
    "\n",
    "- Given context words → predict center word\n",
    "- \"The cat sat on the ___\" → predict \"mat\"\n",
    "\n",
    "2. Skip-gram:\n",
    "\n",
    "- Given center word → predict context words\n",
    "- Given \"cat\" → predict \"the\", \"sat\", \"on\"\n",
    "\n",
    "**Why we do it:**\n",
    "- Learns embeddings from raw text (unsupervised)\n",
    "- Words appearing in similar contexts get similar vectors\n",
    "- Captures semantic and syntactic relationships\n",
    "- Skip-gram: better for rare words, smaller datasets\n",
    "- CBOW: faster, better for frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23df37e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "sentences = [\n",
    "    \"the cat sat on the mat\",\n",
    "    \"the dog sat on the log\",\n",
    "    \"cats and dogs are friends\"\n",
    "]\n",
    "\n",
    "# Tokenize\n",
    "tokenized = [word_tokenize(sent.lower()) for sent in sentences]\n",
    "\n",
    "# Train Word2Vec (skip-gram)\n",
    "model = Word2Vec(sentences=tokenized, vector_size=100, window=5, \n",
    "                 min_count=1, sg=1)  # sg=1 for skip-gram, sg=0 for CBOW\n",
    "\n",
    "# Get embedding for a word\n",
    "vector = model.wv['cat']\n",
    "print(\"Cat vector shape:\", vector.shape)  # (100,)\n",
    "\n",
    "# Find similar words\n",
    "similar = model.wv.most_similar('cat', topn=3)\n",
    "print(\"Similar to cat:\", similar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39272d85",
   "metadata": {},
   "source": [
    "#### **Global Vectors (GLoVe)**\n",
    "\n",
    "**What it is:** Word embedding method that uses global word co-occurrence statistics from a corpus. Unlike Word2Vec which uses local context windows, GloVe looks at the entire corpus's word co-occurrence matrix.\n",
    "\n",
    "**Simplified:** GloVe counts how often words appear near each other across an ENTIRE corpus (like all of Wikipedia), then creates vectors where words that frequently co-occur are close together.\n",
    "\n",
    "**Core Idea:**\n",
    "- Count how often words appear together across entire corpus\n",
    "- \"ice\" appears with \"solid\" more than \"gas\" does\n",
    "- Ratio of co-occurrence probabilities captures meaning\n",
    "\n",
    "**Why we do it:**\n",
    "\n",
    "- Combines benefits of matrix factorization (global stats) and local context (Word2Vec)\n",
    "- Often performs better than Word2Vec on word analogy tasks\n",
    "- Pre-trained on massive corpora (Wikipedia, Common Crawl)\n",
    "- Fixed embeddings you can plug into your model immediately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ceeb223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GloVe is typically used pre-trained, not trained from scratch\n",
    "import numpy as np\n",
    "\n",
    "# Load pre-trained GloVe embeddings\n",
    "def load_glove(file_path):\n",
    "    embeddings = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.array(values[1:], dtype='float32')\n",
    "            embeddings[word] = vector\n",
    "    return embeddings\n",
    "\n",
    "# Download from: https://nlp.stanford.edu/projects/glove/\n",
    "# glove = load_glove('glove.6B.100d.txt')\n",
    "\n",
    "# Using with gensim\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Convert GloVe format to Word2Vec format\n",
    "# glove2word2vec('glove.6B.100d.txt', 'word2vec_format.txt')\n",
    "# model = KeyedVectors.load_word2vec_format('word2vec_format.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d6a2af",
   "metadata": {},
   "source": [
    "#### **FastText**\n",
    "\n",
    "**What it is:** Extension of Word2Vec that represents words as bags of character n-grams instead of treating words as atomic units. Developed by Facebook.\n",
    "\n",
    "**Core Idea:**\n",
    "- Break words into subword units (character n-grams)\n",
    "- \"apple\" → [\"<ap\", \"app\", \"ppl\", \"ple\", \"le>\", \"<apple>\"]\n",
    "- Word embedding = sum of its n-gram embeddings\n",
    "\n",
    "**Why we do it:**\n",
    "\n",
    "- Handles out-of-vocabulary (OOV) words (Word2Vec/GloVe can't)\n",
    "- Good for morphologically rich languages (German, Turkish)\n",
    "- Captures subword information (\"unhappiness\" = \"un\" + \"happy\" + \"ness\")\n",
    "- Useful for typos, rare words, new words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aceaaa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "sentences = [\n",
    "    [\"the\", \"cat\", \"sat\"],\n",
    "    [\"the\", \"cats\", \"are\", \"sitting\"],\n",
    "    [\"unseen\", \"word\"]\n",
    "]\n",
    "\n",
    "# Train FastText\n",
    "model = FastText(sentences=sentences, vector_size=100, window=5, \n",
    "                 min_count=1, min_n=3, max_n=6)  # n-gram range\n",
    "\n",
    "# Get embedding for seen word\n",
    "vector = model.wv['cat']\n",
    "\n",
    "# KEY FEATURE: Get embedding for UNSEEN word (out-of-vocabulary)\n",
    "# Even if \"cats\" wasn't in training, FastText can generate embedding\n",
    "# by combining n-grams: \"ca\", \"at\", \"ts\", etc.\n",
    "oov_vector = model.wv['kittens']  # Works even if never seen before!\n",
    "\n",
    "# Find similar words\n",
    "similar = model.wv.most_similar('cat')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189e6737",
   "metadata": {},
   "source": [
    "### **3. Classical NLP Tasks**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4665cb79",
   "metadata": {},
   "source": [
    "#### **Sentiment Analysis**\n",
    "\n",
    "What it is: Determining the emotional tone/opinion in text. Classify text as positive, negative, or neutral.\n",
    "\n",
    "Why we do it:\n",
    "\n",
    "Understand customer feedback (reviews, surveys)\n",
    "Monitor brand reputation on social media\n",
    "Automate content moderation\n",
    "Market research and opinion mining\n",
    "Common entry-level NLP task, good for learning pipeline\n",
    "\n",
    "Mathematical Intuition: \n",
    "\n",
    "1. **Convert Text to Numbers (e.g. TF-IDF)**\n",
    "\n",
    "\"I love this\" → [0.2, 0.8, 0.5, 0, 0, ...] (vector of word weights)\n",
    "```\n",
    "\n",
    "2. **Learn weights for each word** (during training)\n",
    "```\n",
    "\"love\" → +0.9 (strongly positive)\n",
    "\"hate\" → -0.8 (strongly negative)\n",
    "\"the\" → 0.01 (neutral)\n",
    "```\n",
    "\n",
    "3. **Calculate sentiment score** (dot product)\n",
    "```\n",
    "Sentiment Score = Σ(word_weight × learned_weight)\n",
    "                = (0.2 × 0.1) + (0.8 × 0.9) + (0.5 × 0.05) + ...\n",
    "                = 0.77\n",
    "```\n",
    "\n",
    "4. **Decision boundary**\n",
    "```\n",
    "If score > 0.5 → Positive\n",
    "If score < -0.5 → Negative\n",
    "Else → Neutral\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fce4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Sample data\n",
    "texts = [\n",
    "    \"I love this product, it's amazing!\",\n",
    "    \"Terrible experience, waste of money\",\n",
    "    \"It's okay, nothing special\",\n",
    "    \"Best purchase ever!\",\n",
    "    \"Horrible quality, very disappointed\"\n",
    "]\n",
    "labels = [1, 0, 2, 1, 0]  # 1=positive, 0=negative, 2=neutral\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2)\n",
    "\n",
    "# Vectorize with TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "# Train classifier\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train_vec, y_train)\n",
    "\n",
    "# Predict\n",
    "new_text = [\"This is absolutely wonderful!\"]\n",
    "new_vec = vectorizer.transform(new_text)\n",
    "prediction = clf.predict(new_vec)\n",
    "print(\"Sentiment:\", prediction)  # Output: [1] (positive)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a610ef4",
   "metadata": {},
   "source": [
    "#### **Text Classification**\n",
    "\n",
    "**What it is:** Assigning predefined categories/labels to text documents. Sentiment analysis is one type; this is the broader task.\n",
    "\n",
    "**Examples:**\n",
    "- Spam vs not spam (email filtering)\n",
    "- Topic classification (sports, politics, tech)\n",
    "- Intent classification (customer query routing)\n",
    "- Language detection\n",
    "\n",
    "**Why we do it:**\n",
    "- Organize large document collections automatically\n",
    "- Route customer requests to correct department\n",
    "- Filter content (spam, inappropriate content)\n",
    "- Tag/categorize articles, emails, support tickets\n",
    "- Foundation for many real-world NLP applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07028f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Sample data\n",
    "texts = [\n",
    "    \"The stock market crashed today\",\n",
    "    \"New smartphone released with better camera\",\n",
    "    \"Team wins championship game\",\n",
    "    \"Company reports quarterly earnings\",\n",
    "    \"Latest gadget review and specs\"\n",
    "]\n",
    "labels = ['business', 'tech', 'sports', 'business', 'tech']\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', MultinomialNB())  # Naive Bayes works well for text\n",
    "])\n",
    "\n",
    "# Train\n",
    "pipeline.fit(texts, labels)\n",
    "\n",
    "# Predict\n",
    "new_texts = [\"Apple announces new product launch\"]\n",
    "predictions = pipeline.predict(new_texts)\n",
    "print(predictions)  # Output: ['tech']\n",
    "\n",
    "# Get probabilities\n",
    "probs = pipeline.predict_proba(new_texts)\n",
    "print(probs)  # Shows confidence for each class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc8b37f",
   "metadata": {},
   "source": [
    "#### **Named Entity Recognition**\n",
    "\n",
    "**What it is:** Identifying and classifying named entities in text into predefined categories like person names, organizations, locations, dates, etc.\n",
    "\n",
    "**Common Entity Types:**\n",
    "\n",
    "- PERSON: \"Barack Obama\", \"Marie Curie\"\n",
    "- ORG: \"Google\", \"United Nations\"\n",
    "- LOC: \"Paris\", \"Mount Everest\"\n",
    "- DATE: \"January 2023\", \"yesterday\"\n",
    "- MONEY: \"$100\", \"50 euros\"\n",
    "\n",
    "**Why We Do It**:\n",
    "- Extract structured information from unstructured text\n",
    "- Build knowledge graphs\n",
    "- Information retrieval (find all documents mentioning \"Google\")\n",
    "- Question answering systems\n",
    "- Anonymization (remove/mask person names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20013f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load pre-trained model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"Apple Inc. was founded by Steve Jobs in Cupertino in 1976.\"\n",
    "\n",
    "# Process text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Extract entities\n",
    "for ent in doc.ents:\n",
    "    print(f\"{ent.text} -> {ent.label_}\")\n",
    "\n",
    "# Output:\n",
    "# Apple Inc. -> ORG\n",
    "# Steve Jobs -> PERSON\n",
    "# Cupertino -> GPE (Geo-Political Entity)\n",
    "# 1976 -> DATE\n",
    "\n",
    "# Visualize (in Jupyter)\n",
    "from spacy import displacy\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac12f51",
   "metadata": {},
   "source": [
    "#### **Part-of-Speech (PoS) Tagging**\n",
    "\n",
    "**What it is:** Assigning grammatical categories (noun, verb, adjective, etc.) to each word in a sentence.\n",
    "\n",
    "**Common POS Tags:**\n",
    "\n",
    "NN: Noun (singular)\n",
    "NNS: Noun (plural)\n",
    "VB: Verb (base form)\n",
    "VBD: Verb (past tense)\n",
    "JJ: Adjective\n",
    "RB: Adverb\n",
    "DT: Determiner (the, a, an)\n",
    "\n",
    "**Why we do it:**\n",
    "\n",
    "-  Disambiguate word meaning (\"book\" = noun vs verb)\n",
    "- Feature for other NLP tasks (NER, parsing)\n",
    "- Grammar checking\n",
    "- Information extraction (extract all verbs = actions)\n",
    "- Improve text understanding (syntax structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df085395",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "# Download required data\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('punkt')\n",
    "\n",
    "text = \"The quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "# Tokenize and tag\n",
    "tokens = word_tokenize(text)\n",
    "pos_tags = pos_tag(tokens)\n",
    "\n",
    "print(pos_tags)\n",
    "# Output:\n",
    "# [('The', 'DT'), ('quick', 'JJ'), ('brown', 'JJ'), \n",
    "#  ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), \n",
    "#  ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')]\n",
    "\n",
    "# Using spaCy\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text)\n",
    "\n",
    "for token in doc:\n",
    "    print(f\"{token.text} -> {token.pos_}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
